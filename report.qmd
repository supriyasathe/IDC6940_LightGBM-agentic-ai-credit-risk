---
title: "Explainable Agentic AI for Credit Risk Assessment using LightGBM"
author:
  - name: "Supriya Sathe (Advisor: Dr. Cohen)"
    url: https://github.com/supriyasathe
bibliography: references.bib
link-citations: true
---


## 1. Introduction

A vital role for financial institutions is evaluating credit risk, which has a direct impact on lending choices, portfolio performance, and regulatory compliance. Because they are straightforward and easy to understand, traditional credit scoring techniques like logistic regression have gained widespread use. These conventional models, however, find it difficult to account for nonlinear patterns and complicated borrower behaviors as financial data grows in size, complexity, and heterogeneity.

The predictive accuracy of credit risk modeling has greatly increased as a result of recent developments in machine learning, especially ensemble-based techniques like Gradient Boosting Decision Trees (GBDT). Among these techniques, the Light Gradient Boosting Machine (LightGBM) has shown itself to be a very effective and scalable framework that can deal with imbalanced and high-dimensional credit datasets. Although LightGBM performs well, it is frequently perceived as a "black-box" paradigm, which restricts transparency and undermines confidence among auditors, regulators, and business users.

This paper suggests an Explainable Agentic AI framework for credit risk assessment utilizing LightGBM in order to overcome these issues. The framework combines an intelligent agentic decision layer that analyzes model predictions, produces insightful explanations, and suggests suitable courses of action with SHAP-based explainability. The suggested approach seeks to facilitate responsible and explicable credit decision-making in regulated financial environments by fusing prediction accuracy, transparency, and adaptive reasoning.

---

## 2. Methods



---

## 3. Analysis and Results

Import the project dependencies

<!-- This code chunk is loading all libraries being used in the project. -->

```{r}
#| code-fold: true
#| message: false
#| warning: false
#| output: false

packages <- c("tidyverse", "skimr", "knitr", "ggthemes", "ggrepel", "dslabs")
# install.packages(packages)
# Load Libraries
lapply(packages, library, character.only = TRUE)
# Set seed for reproducibility
set.seed(123)
```


### 3.1. Data Ingestion

This code chunk is importing the dataset from the folder datasets which it is finding at the root of the github repo and it is concatenating the path to run the function read_csv from tidyverse.

```{r}
#| code-fold: true
#| output: false
find_git_root <- function(start = getwd()) {
  path <- normalizePath(start, winslash = "/", mustWork = TRUE)
  while (path != dirname(path)) {
    if (dir.exists(file.path(path, ".git"))) return(path)
    path <- dirname(path)
  }
  stop("No .git directory found â€” are you inside a Git repository?")
}

repo_root <- find_git_root()
datasets_path <- file.path(repo_root, "datasets")

# Reading the datafile credit_risk_dataset.csv
credit_path <- file.path(datasets_path, "credit_risk_dataset.csv")
credit1 = read_csv(credit_path, show_col_types = FALSE)
```


```{r}
summary(credit1)
head(credit1)
# Count total NAs per column
colSums(is.na(credit1))
```

we found that:

- person_emp_length has 895 NA values
- loan_int_rate has 3116 NA values

---

## 4. Conclusion


---

### References

::: {#refs}
:::